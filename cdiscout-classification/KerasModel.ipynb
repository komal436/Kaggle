{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the environment to use standalone spark\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def set_home():\n",
    "    os.environ['SPARK_HOME']=\"C:\\spark\\spark\"\n",
    "    os.environ['JAVA_HOME']=\"C:/Program Files/Java/jre1.8.0_152\"\n",
    "    sys.path.append(\"c:\\spark\\spark\\python\")\n",
    "    os.environ['HADOOP_HOME']=\"c:\\spark\\hadoop\"\n",
    "    sys.path.append(\"C:\\spark\\mongo-hadoop\\spark\\src\\main\\python\")\n",
    "def set_work():\n",
    "    os.environ['SPARK_HOME']=\"D:\\spark\\spark\"\n",
    "    os.environ['JAVA_HOME']=\"C:/Program Files/Java/jre1.8.0_151\"\n",
    "    sys.path.append(\"D:\\spark\\spark\\python\")\n",
    "    os.environ['HADOOP_HOME']=\"D:\\spark\\hadoop\"\n",
    "    sys.path.append(\"D:\\mongo-hadoop\\spark\\src\\main\\python\")\n",
    "    sys.path.append(\"D:\\pyspark-cassandra\\python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_work()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This will help to print all the commands output of a single cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import pymongo_spark\n",
    "pymongo_spark.activate()\n",
    "\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from skimage.data import imread\n",
    "import io\n",
    "import numpy as np\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CNTK backend\n",
      "C:\\Users\\kokumars\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\cntk_backend.py:18: UserWarning: CNTK backend warning: GPU is not detected. CNTK's CPU version is not fully optimized,please run with GPU to get better performance.\n",
      "  'CNTK backend warning: GPU is not detected. '\n"
     ]
    }
   ],
   "source": [
    "# Now we have the train_df, val_df, class_df. We are good to go to start building a keras model using functional API's\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, MaxPooling2D, Dropout, Flatten\n",
    "from keras.utils import to_categorical, np_utils\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.activations import relu, sigmoid\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "try:\n",
    "    sc = SparkContext()\n",
    "except:\n",
    "    print(\"alredy done\")\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = \"C:/Users/kokumars/OneDrive - Microsoft/Kaggle/France-ECommerce/category_names.csv\"\n",
    "\n",
    "customSchema = StructType([ \\\n",
    "    StructField(\"category_id\", IntegerType(), True), \\\n",
    "    StructField(\"category_level1\", StringType(), True), \\\n",
    "    StructField(\"category_level2\", StringType(), True), \\\n",
    "    StructField(\"category_level3\", StringType(), True)])\n",
    "\n",
    "df = sqlcontext.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true') \\\n",
    "    .load(file, schema = customSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map the categories to numeric categories, so we can do one hot encoding.\n",
    "class_df = df.select(['category_id']).sort('category_id').withColumn(\"id\", monotonically_increasing_id())\n",
    "num_classes = class_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map classes to idx and idx to classes for lookups\n",
    "classes = class_df.toPandas()\n",
    "cat2idx = {}\n",
    "idx2cat = {}\n",
    "\n",
    "for ir in classes.itertuples():\n",
    "    category_id = ir[2]\n",
    "    category_idx = ir[1]\n",
    "    cat2idx[category_id] = category_idx\n",
    "    idx2cat[category_idx] = category_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 157 ms\n"
     ]
    }
   ],
   "source": [
    "# BSONFileRDD is coming from the pymongo_spark. It provides API to parse bson files to RDD\n",
    "file = 'C:/Users/kokumars/OneDrive - Microsoft/Kaggle/France-ECommerce/train_example.bson'\n",
    "#file = 'C:/Users/kokumars/Desktop/Kaggle/train.bson'\n",
    "#file = \"D:/Kaggle/France-ECommerce/train.bson\"\n",
    "%time bsonFileRDD = sc.BSONFileRDD(file)\n",
    "# Lets count how many products are present\n",
    "# bsonFileRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flatten the product list over the images of a product.\n",
    "flatMapRDD = bsonFileRDD.map(lambda x: ((x['_id'], x['category_id']), x['imgs'])).flatMapValues(lambda x : x)\n",
    "#fd = flatMapRDD.map(lambda x: Row(id=x[0][0], cid=x[0][1], \\\n",
    "#                                  img = (img_to_array(load_img(io.BytesIO(x[1]['picture'])))).astype('float32')/255.0))\n",
    "\n",
    "fd = flatMapRDD.map(lambda x: Row(id=x[0][0], cid=int(cat2idx[x[0][1]]), img=x[1]['picture']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[24] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = fd.filter(lambda x: 0 < x['id'] < 12000)\n",
    "fd.persist()\n",
    "count = fd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#This is an example to understand how the flattening is done for this problem.\n",
    "data = [{'id':0, 'cid':10879, 'img':[{'pic':'picture'}, {'pic': 'picture1'}]},\n",
    "       {'id':1, 'cid':10872, 'img':[{'pic':'picture'}, {'pic': 'picture1'}]}]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "d = rdd.map(lambda x: ((x['cid'], x['id']), x['img'])).flatMapValues(lambda x : x)\n",
    "#fd = d.map()\n",
    "d.collect()\n",
    "fd = d.map(lambda x: Row(id=x[0][1], cid=x[0][0], img=x[1]['pic']))\n",
    "fd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.57 s\n"
     ]
    }
   ],
   "source": [
    "%time train_df, val_df = fd.toDF().randomSplit([0.7, 0.3], seed=99)\n",
    "train_df = train_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "val_df = val_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "#train_rdd, val_rdd = fd.randomSplit([0.7, 0.3], seed=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height, depth =  img_to_array(load_img(io.BytesIO(train_df.take(1)[0]['img']))).shape\n",
    "width, height, depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(height, width, depth), activation='relu', padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "epochs = 10\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#te = val_df.filter((0 <= val_df.id) & (val_df.id < 100)).select(['cid', 'img']).toPandas()\n",
    "#dt = te['img']\n",
    "#t = val_df.repartition(10, \"id\", \"cid\")\n",
    "#t.show()\n",
    "#t = bsonFileRDD.repartition(100)\n",
    "#t.persist()\n",
    "#t.count()\n",
    "#fd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "import math\n",
    "class classSeq(Sequence):\n",
    "    def __init__(self, sqlc, df, batch_size, count): \n",
    "        self.sql_context = sqlc        \n",
    "        self.count = count\n",
    "        self.batch_size = batch_size        \n",
    "        self.iter = 0\n",
    "        self.len = self.__len__()\n",
    "        self.df = df\n",
    "        self.start_idx = 0\n",
    "        self.end_idx = batch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return math.ceil(self.count / self.batch_size)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.iter = 0\n",
    "        self.start_idx = 0\n",
    "        self.end_idx = self.batch_size\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.iter < self.len:\n",
    "            start_idx = self.start_idx\n",
    "            end_idx = self.end_idx            \n",
    "            #param = sc.broadcast(self.start_idx, self.end_idx, self.df)\n",
    "            #print(self.count, self.iter, self.len, param.value, self.start_idx, self.end_idx)\n",
    "            #fdf = self.df.filter((param.value[0] <= param.value[2].id) & (param.value[2].id < param.value[1])).select(['cid', 'img']).toPandas()\n",
    "            fdf = self.df.filter((start_idx <= self.df.id) & (self.df.id < end_idx)).select(['cid', 'img']).toPandas()\n",
    "            self.start_idx = self.start_idx + self.batch_size\n",
    "            self.end_idx = self.end_idx + self.batch_size\n",
    "            self.iter = self.iter + 1\n",
    "            #fdf = fdf.map(lambda x: [cat2idx[x['cid']], x['img']]).toDF().toPandas()\n",
    "            y_train = to_categorical(fdf['cid'], num_classes=num_classes)\n",
    "            x_train = np.array([img_to_array(load_img(io.BytesIO(x))).astype('float32')/255.0 for x in fdf['img']])\n",
    "            #x_train = dataPandas['_2'].apply(lambda x: np.array(img_to_array(load_img(io.BytesIO(x))).astype('float32')/255.0).astype('float32'))\n",
    "            #x_train = np.array(frdd.map(lambda x: ((img_to_array(load_img(io.BytesIO(x['img'])))).astype('float32')/255.0).astype('float32')).collect())\n",
    "            #y_train = to_categorical(np.array(frdd.map(lambda x: cat2idx[x['cid']]).collect()), num_classes=num_classes)\n",
    "            return (x_train, y_train)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 7069896, 100000\n",
    "#train_count = train_df.count()\n",
    "#val_count = val_df.count()\n",
    "#train_count = 7069896\n",
    "#val_count = 3069896\n",
    "train_count = count * 0.7\n",
    "val_count = count * 0.3\n",
    "train_seq = classSeq(sqlcontext, train_df, 1000, train_count)\n",
    "val_seq = classSeq(sqlcontext, val_df, 700, val_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "%time scores = model.fit_generator(train_seq, steps_per_epoch = len(train_seq), max_queue_size=100, workers = 10, use_multiprocessing=False,epochs=2)\n",
    "#%time model.fit(q['_1'], q['_2'], epochs=epochs, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss = model.evaluate_generator(val_seq, steps =len(val_seq), max_queue_size=100, workers=2, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "#%time scores = model.evaluate_generator(val_generator(), steps=2)\n",
    "len(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.get_weights()\n",
    "loss[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
