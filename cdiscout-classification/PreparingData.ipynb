{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Program Files\\\\Java\\\\jdk1.8.0_131'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"JAVA_HOME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the environment to use standalone spark\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def set_home():\n",
    "    os.environ['SPARK_HOME']=\"C:\\spark\\spark\"\n",
    "    os.environ['JAVA_HOME']=\"C:/Program Files/Java/jre1.8.0_152\"\n",
    "    sys.path.append(\"c:\\spark\\spark\\python\")\n",
    "    os.environ['HADOOP_HOME']=\"c:\\spark\\hadoop\"\n",
    "    sys.path.append(\"C:\\spark\\mongo-hadoop\\spark\\src\\main\\python\")\n",
    "def set_work():\n",
    "    os.environ['SPARK_HOME']=\"D:/spark/spark\"\n",
    "    #os.environ['JAVA_HOME']=\"C:/Program Files/Java/jre1.8.0_151\"\n",
    "    sys.path.append(\"D:\\spark\\spark\\python\")\n",
    "    os.environ['HADOOP_HOME']=\"D:\\spark\\hadoop\"\n",
    "    sys.path.append(\"D:\\mongo-hadoop\\spark\\src\\main\\python\")\n",
    "    \n",
    "set_work()\n",
    "#set_home()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This will help to print all the commands output of a single cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "#import pyspark_cassandra\n",
    "#from pyspark_cassandra import CassandraSparkContext\n",
    "import pymongo_spark\n",
    "pymongo_spark.activate()\n",
    "\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from skimage.data import imread\n",
    "import io\n",
    "import numpy as np\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.171.23.123:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.171.23.123:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1d6a2616470>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    conf = SparkConf(True).set(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "    sc =  CassandraSparkContext(conf=conf)\n",
    "except:\n",
    "    pass\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "file = \"C:/Users/kokumars/OneDrive - Microsoft/Kaggle/France-ECommerce/category_names.csv\"\n",
    "\n",
    "customSchema = StructType([ \\\n",
    "    StructField(\"category_id\", IntegerType(), True), \\\n",
    "    StructField(\"category_level1\", StringType(), True), \\\n",
    "    StructField(\"category_level2\", StringType(), True), \\\n",
    "    StructField(\"category_level3\", StringType(), True)])\n",
    "\n",
    "df = sqlContext.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true') \\\n",
    "    .load(file, schema = customSchema)\n",
    "# map the categories to numeric categories, so we can do one hot encoding.\n",
    "class_df = df.select(['category_id']).sort('category_id').withColumn(\"id\", monotonically_increasing_id())\n",
    "num_classes = class_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5270"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"C:/Users/kokumars/OneDrive - Microsoft/Kaggle/France-ECommerce/category_names.csv\"\n",
    "df = pd.read_csv(file, header='infer')\n",
    "# map the categories to numeric categories, so we can do one hot encoding.\n",
    "df = df.sort_values('category_id')\n",
    "df.insert(0, 'id', range(len(df)))\n",
    "df = df[['id', 'category_id']]\n",
    "num_classes = len(df)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat2idx = {}\n",
    "idx2cat = {}\n",
    "\n",
    "for ir in df.itertuples():\n",
    "    category_id = ir[2]\n",
    "    category_idx = ir[1]\n",
    "    cat2idx[category_id] = category_idx\n",
    "    idx2cat[category_idx] = category_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1863"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "27945"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BSONFileRDD is coming from the pymongo_spark. It provides API to parse bson files to RDD\n",
    "#file = 'C:/Users/kokumars/OneDrive - Microsoft/Kaggle/France-ECommerce/train_example.bson'\n",
    "#file = 'C:/Users/kokumars/Desktop/Kaggle/train.bson'\n",
    "file = \"D:/Kaggle/France-ECommerce/train.bson\"\n",
    "config = None\n",
    "%time rdd = sc.BSONFileRDD(file)\n",
    "# Lets count how many products are present\n",
    "# bsonFileRDD.count()\n",
    "p = rdd.getNumPartitions()\n",
    "#bsonFileRDD = bsonFileRDD.repartition(p)\n",
    "rdd.getNumPartitions()\n",
    "bsonFileRDD = rdd.repartition(p * 15)\n",
    "bsonFileRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flatten the product list over the images of a product.\n",
    "flatMapRDD = bsonFileRDD.map(lambda x: ((x['_id'], x['category_id']), x['imgs'])).flatMapValues(lambda x : x)\n",
    "#fd = flatMapRDD.map(lambda x: Row(id=x[0][0], cid=x[0][1], \\\n",
    "#                                  img = (img_to_array(load_img(io.BytesIO(x[1]['picture'])))).astype('float32')/255.0))\n",
    "\n",
    "fd = flatMapRDD.map(lambda x: Row(idx = x[0][0], cid =int(cat2idx[x[0][1]]), img = x[1]['picture']))\n",
    "fd = fd.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = fd.toDF()\n",
    "#_max = df.select('idx').rdd.max()\n",
    "#_min = df.select('idx').rdd.min()\n",
    "#_count = df.count()\n",
    "\n",
    "df.registerTempTable(\"df_table\")\n",
    "t = sqlContext.sql(\"SELECT MAX('idx') as maxval FROM df_table\").collect() #[0].asDict()['maxval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, val = fd.randomSplit([0.7, 0.3], seed=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.171.23.123:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.171.23.123:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://10.171.23.123:7077 appName=PySparkShell>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_df = train_df.withColumn(\"idx\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#val_df = val_df.withColumn(\"idx\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bad while running to put data to cassandra. Using different Data Model.\n",
    "#train_df = train_df.sort('cid').withColumn(\"id\", monotonically_increasing_id())\n",
    "#val_df = val_df.sort('cid').withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def pandas_factory(colnames, rows):\n",
    "    return pd.DataFrame(rows, columns=colnames)\n",
    "\n",
    "session.row_factory = pandas_factory\n",
    "session.default_fetch_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keyspace = \"komal\"\n",
    "try:\n",
    "    query = f\"DROP KEYSPACE IF EXISTS {keyspace}\"\n",
    "    session.execute(query)\n",
    "except:\n",
    "    print(\"Drop keyspace {keyspace} failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    #query = \"DROP KEYSPACE komal1;\"\n",
    "    #session.execute(query);\n",
    "    query = \"CREATE KEYSPACE IF NOT EXISTS komal WITH replication = {'class':'SimpleStrategy', 'replication_factor':1};\"    \n",
    "    session.execute(query);\n",
    "except:\n",
    "    print(\"Create keyspace already exists\")\n",
    "session.execute(f\"USE {keyspace};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    drop_table = \"DROP TABLE IF EXISTS train\"    \n",
    "    session.execute(drop_table)\n",
    "except:\n",
    "    print(\"Drop Table train failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    drop_table = \"DROP TABLE IF EXISTS val\"\n",
    "    session.execute(drop_table)\n",
    "except:\n",
    "    print(\"Drop table val failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    drop_table = \"DROP TABLE IF EXISTS test_val\"\n",
    "    session.execute(drop_table)\n",
    "except:\n",
    "    print(\"Drop table test_val failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    drop_table = \"DROP TABLE IF EXISTS test_train\"\n",
    "    session.execute(drop_table)\n",
    "except:\n",
    "    print(\"Drop table test_train failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    #\"CREATE TABLE t (pk int, t int, v text, s text, PRIMARY KEY (pk, t));\n",
    "    create_table = \"CREATE TABLE train (cid BIGINT, img BLOB, PRIMARY KEY (cid, img));\"                                        \n",
    "    session.execute(create_table)\n",
    "except:\n",
    "    print(\"create table train failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    create_table = \"CREATE TABLE val (cid BIGINT, img BLOB, PRIMARY KEY (cid, img));\"\n",
    "    session.execute(create_table)\n",
    "except:\n",
    "    print(\"Create Table Val failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    create_table = \"CREATE TABLE test_val (cid BIGINT, img BLOB, PRIMARY KEY (cid));\"\n",
    "    session.execute(create_table)\n",
    "except:\n",
    "    print(\"Create Table Val failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    create_table = \"CREATE TABLE test_train (cid BIGINT, img BLOB, PRIMARY KEY (cid));\"\n",
    "    session.execute(create_table)\n",
    "except:\n",
    "    print(\"Create Table test_train failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.set_keyspace(keyspace)\n",
    "#result = session.execute(\"SELECT * FROM train;\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fd.saveToCassandra(\"\")\n",
    "\n",
    "rdd = sc.cassandraTable(keyspace, \"train\")\n",
    "rdd.count()\n",
    "#session.execute(\"SELECT * FROM train\")\n",
    "#df.rdd.saveToCassandra(\"kaggle\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use it when u want to overwrite the entries .option(\"confirm.truncate\", True) \\\n",
    "# Inputsize/ Records 58.2 GB / 7069896 *0.7\n",
    "#train records: 8660954 \n",
    "train_df.write\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .mode('append') \\\n",
    "    .option(\"table\", \"train\") \\\n",
    "    .option(\"keyspace\", \"komal\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input Size/Records 53.2 GB / 7069896 *0.3\n",
    "#val_df records: 3710339\n",
    "val_df.write\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .mode('append') \\\n",
    "    .option(\"table\", \"val\") \\\n",
    "    .option(\"keyspace\", \"komal\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.write\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .mode('overwrite') \\\n",
    "    .option(\"table\", \"test_train\") \\\n",
    "    .option(\"keyspace\", \"kaggle\")\\\n",
    "    .option(\"confirm.truncate\", True) \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_df.write\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .mode('overwrite') \\\n",
    "    .option(\"table\", \"test_val\") \\\n",
    "    .option(\"keyspace\", keyspace)\\\n",
    "    .option(\"confirm.truncate\", True) \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "create_table = \"\"\"CREATE TABLE train (\n",
    "    id BIGINT,\n",
    "    cid BIGINT,\n",
    "    img BLOB,\n",
    "    PRIMARY KEY (id, cid));\"\"\"\n",
    "session.execute(create_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "8660954 + 3710339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def pandas_factory(colnames, rows):\n",
    "    return pd.DataFrame(rows, columns=colnames)\n",
    "\n",
    "session.row_factory = pandas_factory\n",
    "session.default_fetch_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_min = 20\n",
    "_max = 30\n",
    "query = f\"SELECT * FROM val WHERE id < {_max} and id > {_min} LIMIT 100 ALLOW FILTERING;\"\n",
    "query\n",
    "%time result = session.execute(query=query, timeout=50000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdf = result._current_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark_cassandra\n",
    "rdd = sc.cassandraTable(\"kaggle\", \"train\", row_format= pyspark_cassandra.RowFormat.ROW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is not effecient \n",
    "#rdd.filter(\"id < 100\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS CATEGORY( \\\n",
    "                category_id BIGINT, \\\n",
    "                category_level1 BIGINT, \\\n",
    "                category_level2 BIGINT, \\\n",
    "                category_level3 BIGINT, \\\n",
    "                PRIMARY KEY (category_id));\"\n",
    "session.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.count()\n",
    "#val_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#s = pdf['cid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "j = s.map(lambda x: 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = session.execute(\"SELECT * FROM TEST_TRAIN;\", timeout=50000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#result.current_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#result._current_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.cassandraTable(\"kaggle\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf = df.select('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c= df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = rdd.map(lambda x: (x['id'], 1))\n",
    "#.groupbykey(lambda a,b : a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t.reduceByKey(lambda a,b : a+b).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.cassandraTable(\"kaggle\", \"test_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd.toDF().sort('idx').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.cassandraTable(\"kaggle\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd.cassandraCount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"select count(*) from train;\"\n",
    "result = session.execute(query, timeout=5000000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.current_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"select MAX(cid) from train;\"\n",
    "result = session.execute(query, timeout=50000000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.current_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"select MIN(cid) from train;\"\n",
    "result = session.execute(query, timeout=50000000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.current_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"select count(*) from train where cid > 4000 and cid < 5269 ALLOW FILTERING;\"\n",
    "result = session.execute(query, timeout=50000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(result.current_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"select * from train where idx > 5000 and idx <= 6000 ALLOW FILTERING;\"\n",
    "result = session.execute(query, timeout=50000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.current_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rd = sc.parallelize(fd.take(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rd.toDF().withColumn(\"idx\", monotonically_increasing_id()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pt = train.getNumPartitions()\n",
    "pv = val.getNumPartitions()\n",
    "train = train.repartition(pt * 9)\n",
    "val = val.repartition(pv * 9)\n",
    "train.getNumPartitions()\n",
    "val.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_work():\n",
    "    return \"file:///komal1234/Kaggle/test2/\"\n",
    "def set_home():\n",
    "    return \"C:/Users/kokumars/Desktop/Kaggle/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kokumars'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getpass\n",
    "getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.saveAsTextFile(set_work())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val.saveAsTextFile(\"C:/Users/kokumars/Desktop/Kaggle/val\", \"org.apache.hadoop.io.compress.GzipCodec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = \"C:/Users/kokumars/Desktop/Kaggle/train/part-00000*\"\n",
    "rdd = sc.textFile(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = \"C:/Users/kokumars/Desktop/Kaggle/train_test\"\n",
    "train.saveAsTextFile(file, \"org.apache.hadoop.io.compress.GzipCodec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = \"C:/Users/kokumars/Desktop/Kaggle/train_test/part-00000.*\"\n",
    "rdd = sc.textFile(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "def ev(x):\n",
    "    val = eval(x).asDict()\n",
    "    return Row(idx = val['idx'], cid=val['cid'], img=val['img'])\n",
    "    \n",
    "r = rdd.map(ev)\n",
    "\n",
    "r.toDF().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = val.asDict()\n",
    "x['idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"komal{0:05d}\".format(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
